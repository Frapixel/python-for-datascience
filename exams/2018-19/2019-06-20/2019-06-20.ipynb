{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d53b65d29a9d12df65ccee53663ce881",
     "grade": false,
     "grade_id": "header",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# 2019-06-20 Exam\n",
    "\n",
    "### General Instructions:\n",
    "\n",
    "Welcome to the **Python Programming (for Data Science)** exam session! Please, read **carefully** the instructions below before start writing code. \n",
    "\n",
    "This session will last **75 minutes** and is divided into **two parts**: one about \"general\" Python programming and the other about Python programming for Data Science. Each part is made of a set of exercises, which globally accounts for **16** + **16** = **32 points**.\n",
    "You will earn all of the points associated to an exercise **if and only if** the answer you provide passes successfully **all** the tests (both those that are visible and those that are hidden to you).<br />\n",
    "\n",
    "To actually write down your implementation, make sure to fill in any place that says <code style=\"color:green\">**_# YOUR CODE HERE_**</code>. Note also that you should **either comment or delete** any <code style=\"color:green\">**raise NotImplementedError()**</code> exception.<br />\n",
    "\n",
    "For this exam session **you will not be allowed** to use any lecture material yet you will be able to access the following APIs:\n",
    "\n",
    "-  [Python](https://docs.python.org/3.6/library/index.html)\n",
    "-  [Numpy](https://docs.scipy.org/doc/numpy-1.13.0/reference/)\n",
    "-  [Scipy](https://docs.scipy.org/doc/scipy-1.0.0/reference/)\n",
    "-  [Pandas](https://pandas.pydata.org/pandas-docs/version/0.22/api.html)\n",
    "-  [Matplotlib](https://matplotlib.org/2.1.1/api/index.html)\n",
    "-  [Seaborn](http://seaborn.pydata.org/api.html)\n",
    "\n",
    "Once you are done, save this notebook and rename it as follows:\n",
    "\n",
    "<code>**YOURUSERNAME_2019-06-20.ipynb**</code>\n",
    "\n",
    "where <code>**YOURUSERNAME**</code> is your actual username. To be consistent, we are expecting your username to be composed by your first name's initial, followed by your full lastname. As an example, in my case this notebook must be saved as <code>**gtolomei_2019-06-20.ipynb**</code> (Remember to insert an underscore <code>**'_'**</code> between your username and the date).<br />\n",
    "\n",
    "Finally, go back to the [Moodle](https://elearning.studenti.math.unipd.it/esami/mod/assign/view.php?id=528) web page of the \"**2019-06-20 Python Programming Exam**\"; there, you will be able to upload your notebook file for grading.\n",
    "\n",
    "<center><h3>Submissions are allowed until <span style=\"color:red\">Thursday, 20 June 2019 at 10:45 AM</span></h3></center>\n",
    "\n",
    "Note that there is no limit on the number of submissions; however, be careful when you upload a new version of this notebook because each submission overwrites the previous one. \n",
    "The due date indicated above is **strict**; after that, the system will not accept any more submissions and the latest uploaded notebook will be the one considered for grading.\n",
    "\n",
    "The archive you have downloaded (<code style=\"color:magenta\">**2019-06-20-exam.tar**</code>) is orgaized as follows:\n",
    "\n",
    "<code style=\"color:red\">**2019-06-20-exam**</code> (root)<br />\n",
    "|----<code style=\"color:green\">**2019-06-20.ipynb**</code> (_this_ notebook)<br />\n",
    "|----<code>**corpus.txt**</code> (the text corpus you will be using for answering general Python programming questions)<br />\n",
    "|----<code>**dataset.csv**</code> (the dataset you will be using for answering data science related questions)<br />\n",
    "|----<code>**README.txt**</code> (a description of the dataset above)\n",
    "\n",
    "<center><h3>... Now, sit back, relax, and do your best!</h3></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**First Name** = Your _first name_ here\n",
    "\n",
    "**Last Name** = Your _last name_ here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a4372d5c9bac76ada487fa9d20c08cc8",
     "grade": false,
     "grade_id": "import",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Adding the following line, allows Jupyter Notebook to visualize plots\n",
    "# produced by matplotlib directly below the code cell which generated those.\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from nose.tools import assert_equal\n",
    "from operator import itemgetter\n",
    "\n",
    "EPSILON = .0000001 # tiny tolerance for managing subtle differences resulting from floating point operations\n",
    "\n",
    "TEXT_CORPUS_FILE = \"corpus.txt\"\n",
    "DATASET_FILE = \"dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ec33bddf64f15258e1d3a4acdcdc65c5",
     "grade": false,
     "grade_id": "part-1",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 1: General Coding (16 points)\n",
    "\n",
    "For **Part 1**, you will be asked to use the list below - called <code>**corpus**</code> - which contains a list of text documents, where each document is represented by a lowercase string with no punctuation character whatsoever.<br /> \n",
    "Please, execute the cell right below to successfully load those documents into <code>**corpus**</code>, see a few sample documents, and then answer the following questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c778613a74d06b626093533b8105857f",
     "grade": false,
     "grade_id": "part-1-required",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# used to replace any punctuation symbol with an empty character ('')\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "# load each individual document as a lowercase string into the list of strings `corpus`\n",
    "corpus = [\" \".join(doc.strip().lower().translate(translator).split()) for doc in open(TEXT_CORPUS_FILE)]\n",
    "# print out the first 5 documents loaded\n",
    "print(\"The following are the first 5 documents loaded out of a total of {} documents:\\n\".format(len(corpus)))\n",
    "print(\"\\n\".join(corpus[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2e5cf5bbb8c76fbf431287c28e8e24f9",
     "grade": false,
     "grade_id": "exercise-1-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.1 (1 point)\n",
    "\n",
    "Implement the function <code>**longest_doc_id**</code>, which returns the **id** of the document with the largest length among those calculated across all the documents in the <code>**corpus**</code>.<br />\n",
    "We define the _length_ of a document as the number of the tokens which the document string is made of; a _token_ is any substring which is separated from the others by a **whitespace character**, i.e., <code>**\" \"**</code>.\n",
    "\n",
    "(**EXAMPLE:** If the document you are working with is the string <code>**\"I think therefore I am\"**</code>, then the corresponding tokens will be: <code>**\"I\"**</code>, <code>**\"think\"**</code>, <code>**\"therefore\"**</code>, <code>**\"I\"**</code>, and <code>**\"am\"**</code> thereby the length of this document will be **5**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7b2d6ee2f856b3b99a74a36edbb9e9fb",
     "grade": false,
     "grade_id": "exercise-1-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def longest_doc_id():\n",
    "    \"\"\"\n",
    "    Returns the id of the longest document among all the documents in the `corpus`.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ac26f5110723ae96d7d3c35b3646d756",
     "grade": true,
     "grade_id": "exercise-1-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `longest_doc_id` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(True, longest_doc_id() > 12)\n",
    "assert_equal(True, longest_doc_id() < 246)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2ee24052231aff98a3a11d9bad893447",
     "grade": false,
     "grade_id": "exercise-1-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2 (3 points)\n",
    "\n",
    "Implement the function <code>**doc_stats**</code>, which returns a custom data structure, i.e., dictionary, where each key is a <code>**doc_id**</code> and each value is a tuple containing the <code>**min**</code>, <code>**max**</code>, <code>**mean**</code>, and <code>**standard deviation**</code> (in this very specific order) of the number of characters of the words which _that_ <code>**doc_id**</code> is made of.<br />\n",
    "For example, if the document is <code>\"I have been to Chargoggagoggmanchauggagoggchaubunagungamaugg lake last summer\"</code>, then:\n",
    "-  <code>**min = 1**</code>\n",
    "-  <code>**max = 45**</code>\n",
    "-  <code>**mean = 8.75**</code>\n",
    "-  <code>**std_dev = 13.77**</code>\n",
    "\n",
    "(**NOTE:** The _Chargoggagoggmanchauggagoggchaubunagungamaugg_ lake truly exists, and is located in Webster, Massachussets, USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d6528f1a7540cd2fe1842a34fd54f189",
     "grade": false,
     "grade_id": "exercise-1-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def doc_stats():\n",
    "    \"\"\"\n",
    "    Returns a dictionary where each key is a `doc_id` and each value is a tuple containing \n",
    "    the min, max, mean, and standard deviation of the number of characters of each word for that document.\n",
    "    \"\"\"\n",
    "    doc_stats = {} # This is the variable that needs to be returned\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return doc_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "35d48d8d04bdedc1cab629c8219aa79d",
     "grade": true,
     "grade_id": "exercise-1-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `doc_stats` function\n",
    "\"\"\"\n",
    "\n",
    "# Call off the function implemented above\n",
    "stats = doc_stats()\n",
    "\n",
    "# Tests\n",
    "assert_equal(2, stats[0][0])\n",
    "assert_equal(11, stats[0][1])\n",
    "assert_equal(True, np.abs(7.25 - stats[0][2]) < EPSILON)\n",
    "assert_equal(True, np.abs(3.2691742076555053 - stats[0][3]) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1bdf667d9c641d77832f2b1d7cc6405e",
     "grade": false,
     "grade_id": "exercise-1-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.3 (5 points)\n",
    "\n",
    "Implement the function <code>**words_smaller_than**</code>, which takes as input two integers <code><b>k</b></code> and <code>**doc_id**</code>, and returns the number of words in <code>**doc_id**</code> whose length (i.e., number of characters) is **strictly smaller** than <code><b>k</b></code>.<br />\n",
    "By convention, documents are identified by their index position in the <code>**corpus**</code> list, therefore the first element of the list will correspond to <code>**doc_id = 0**</code>, the second to <code>**doc_id = 1**</code>, and so on and so forth. As such, if the input <code>**doc_id**</code> is outside of its valid range $[0, N-1]$ (where $N$ is the total number of documents in the <code>**corpus**</code> list), the function should immediately return <code>**-1**</code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "80f736ffaf4f772d22e4100e3e368598",
     "grade": false,
     "grade_id": "exercise-1-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def words_smaller_than(k, doc_id):\n",
    "    \"\"\"\n",
    "    Returns the number of unique words in `doc_id` whose length is strictly smaller than `k` characters. \n",
    "    If the input `doc_id` is outside of its valid range [0, N-1] (where N is the total number of documents in the corpus list), \n",
    "    the function should immediately return -1\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "eba07e4ac2a939212ff20d2a09e7a16e",
     "grade": true,
     "grade_id": "cell-1-3-test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `words_smaller_than` function\n",
    "\"\"\"\n",
    "\n",
    "# Tests\n",
    "assert_equal(2, words_smaller_than(3, 0))\n",
    "assert_equal(0, words_smaller_than(1, 31))\n",
    "assert_equal(10, words_smaller_than(11, 42))\n",
    "assert_equal(8, words_smaller_than(8, 73))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "6784cce770dbae1c08eac55fb8376258",
     "grade": false,
     "grade_id": "exercise-1-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.4 (7 points)\n",
    "\n",
    "In this exercise, you will train a simplified *unigram language model* **for each** document contained in the <code><b>corpus</b></code> collection. Having one model for each document, we will in turn use it to compute a ranked list of the documents which are most likely \"similar\" to a given input query (i.e., a string).<br />\n",
    "In a nutshell, the *unigram language model* needs you to compute the probabilty associated with each word in the document, **disregarding any other surrounding words** (i.e., no contextual information is used to estimate such a probability). More specifically, assuming $V = \\{w_1, \\ldots, w_{N_d}\\}$ is the vocabulary of $N_d$ words extracted from the document $d$, then its associated model $M_d$ will be computed as follows:\n",
    "\n",
    "$$\n",
    "M_d = [P(w_1), \\ldots, P(w_{N_d})]\n",
    "$$\n",
    "where:\n",
    "$$\n",
    "P(w_i) = \\frac{\\text{count}(w_i, d)}{\\sum_{w_j\\in d}\\text{count}(w_j, d)}\n",
    "$$\n",
    "\n",
    "Eventually, we can train several models $M_1, \\ldots, M_N$, i.e., one for each document in our corpus.\n",
    "\n",
    "Once all those models are in place, you will be able to implement the function <code><b>get_most_similar_docs</b></code>, which takes as input a string <code><b>query</b></code> and an integer <code><b>k</b></code>, and returns an **ordered list of** <code><b>k</b></code> **pairs**. Each element of such a list is a tuple made of $(doc_d, prob)$, where $prob$ is the probability of the query $q = t_1\\ldots t_m$ given the trained unigram language model associated with document $d$, i.e., $P(q = t_1\\ldots t_m~|~M_d)$, which can be computed as follows:\n",
    "\n",
    "$$\n",
    "prob(q~|~M_d) = \\prod_{i=1}^k P(t_i~|~M_d)\n",
    "$$\n",
    "\n",
    "To ease this step, you are already given with the function <code><b>compute_probability</b></code>, which takes as input a query $q$ (i.e., a list of words) and a document model $M_d$, and returns the probability $P(q~|~M_d)$ as described above.\n",
    "\n",
    "Finally, the list of pairs returned must be sorted by non-increasing values of $prob$ (i.e., the first element will be the pair containing the document id which most-likely generated the query, and so on...)\n",
    "\n",
    "(**SUGGESTION:** Try solving this exercise using a \"top-down\" approach)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "e4d61c86361c4980ce97d84d26f43b1d",
     "grade": false,
     "grade_id": "exercise-1-4-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_probability(query, model):\n",
    "    \"\"\"\n",
    "    Returns the probability of a query (i.e., a sequence of words) given a certain document model\n",
    "    \"\"\"\n",
    "    query_probability_log_sum = np.sum([np.log(model[word]) if word in model else np.log(EPSILON)\n",
    "                                       for word in query.split(\" \")])\n",
    "    return np.exp(query_probability_log_sum)\n",
    "    \n",
    "\n",
    "def train_unigram_language_model(doc):\n",
    "    \"\"\"\n",
    "    Returns the unigram language model M trained from a single document d\n",
    "    M_d = {w_1: P(w_1 | M_d), ..., w_{N_d}: P(w_{N_d} | M_d)}\n",
    "    where P(w_i | M_d) = count(w_i, d)/sum_{j}count(w_j, d)\n",
    "    \"\"\"\n",
    "    model = {} # dictionary to be populated and returned\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return model\n",
    "\n",
    "def train_unigram_language_models(corpus):\n",
    "    \"\"\"\n",
    "    Returns the global dictionary containing all the unigram language models for all documents of the corpus\n",
    "    {doc_1: M_1, ..., doc_N: M_N}\n",
    "    \"\"\"\n",
    "    models = {} # dictionary to be populated and returned\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return models\n",
    "\n",
    "def get_most_similar_docs(query, k):\n",
    "    \"\"\"\n",
    "    Returns the ordered list of k pairs [(doc_1, prob_1), ..., (doc_k, prob_k)]\n",
    "    such that P(query | M_1) >= P(query | M_2) >= ... >= P(query | M_k)\n",
    "    where M_i is the unigram language model generated from document i\n",
    "    \"\"\"\n",
    "    # 1. Build a dictionary containing all the models indexed by doc_id, as follows {doc_1: M_1, ..., doc_N: M_N}\n",
    "    models = None\n",
    "    # 2. Use this data structure in combination with the `compute_probability` function to construct the list of ordered pairs\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f5fd8754750950385177f5f9555b5954",
     "grade": true,
     "grade_id": "exercise-1-4-test",
     "locked": true,
     "points": 7,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_most_similar_docs` function\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(319, get_most_similar_docs(\"network design\", 10)[0][0])\n",
    "assert_equal(True, np.abs(0.015625000000000007 - get_most_similar_docs(\"network design\", 10)[1][1]) < EPSILON)\n",
    "assert_equal(240, get_most_similar_docs(\"network design\", 10)[9][0])\n",
    "assert_equal(True, np.abs(2.2222222222222197e-08 - get_most_similar_docs(\"network design\", 10)[6][1]) < EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1db87e8d117165548c9344342c6312e0",
     "grade": false,
     "grade_id": "part-2",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Part 2: Data Science (16 points)\n",
    "\n",
    "In this part, you will be working with the dataset file <code>**dataset.csv**</code>. For a complete description of this data source, please refer to the <code>**README.txt**</code> file included in the archive.\n",
    "In a nutshell, this dataset contains **1781** unique (anonymised) URLs, along with a set of **18 features** and a **binary class label** (<code>**TYPE**</code>), which indicates whether the corresponding URL is malicious (<b>1</b>) or not (<b>0</b>).<br />\n",
    "The cell below is responsible for correctly loading the dataset from the <code>**dataset.csv**</code> file. Once this is executed, you can start answering the questions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d786fc676af5dec8022ea29c4b3a0d54",
     "grade": false,
     "grade_id": "part-2-required",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# Load the dataset stored at `DATASET_FILE` using \",\" as field separator and '?' to detect NAs\n",
    "\n",
    "data = pd.read_csv(DATASET_FILE, \n",
    "                   sep=',',\n",
    "                   na_values='?')\n",
    "\n",
    "print(\"Loaded `websites` dataset into a dataframe of size ({} x {})\".format(data.shape[0], data.shape[1]))\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8fae171417f4da04118319971e60e04b",
     "grade": false,
     "grade_id": "exercise-2-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.1 (1 point)\n",
    "\n",
    "Implement the function <code>**get_the_longest_content**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object, and returns the record (i.e., the <code>**pandas.Series**</code>) corresponding to the URL with the longest HTTP header (i.e., the <code>**CONTENT_LENGTH**</code> field) in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "4beea1767cbf75cc99728df72fe94a5e",
     "grade": false,
     "grade_id": "exercise-2-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_the_longest_content(data):\n",
    "    \"\"\"\n",
    "    Returns the record corresponding to the URL with the longest HTTP header in the dataset\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f71f4ba68ec78a2dfb5600c65392d8cc",
     "grade": true,
     "grade_id": "exercise-2-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_the_longest_content` function\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(\"B0_834\", get_the_longest_content(data)[\"URL\"].iloc[0])\n",
    "assert_equal(\"utf-8\", get_the_longest_content(data)[\"CHARSET\"].iloc[0])\n",
    "assert_equal(0, get_the_longest_content(data)[\"SOURCE_APP_PACKETS\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "bc5e0ca5453086afac5acfbd464c2026",
     "grade": false,
     "grade_id": "exercise-2-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.2 (3 points)\n",
    "\n",
    "Implement the function <code>**app_bytes_stats**</code> below. This takes as input a <code>**pandas.DataFrame**</code> object and returns a tuple containing the min, max, avg, and median value of <code>**APP_BYTES**</code> feature, yet computed on a _slice_ of the input <code>**pandas.DataFrame**</code>.<br />\n",
    "The sliced dataset represents the subpopulation containing only **malicious** URLs whose length is **strictly above the overall median**, and whose content length ranges in $[200, 1600)$ bytes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "860a8d74d4db607cbe9c0a6a28050909",
     "grade": false,
     "grade_id": "exercise-2-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def app_bytes_stats(data):\n",
    "    \"\"\"\n",
    "    Returns a tuple containing the min, max, avg, and median value of `APP_BYTES` feature,\n",
    "    yet limited to a slice of the input DataFrame (data). \n",
    "    In particular, this slice will contain instances referring only to malicious URLs\n",
    "    whose length is strictly above the overall median, and whose content length ranges in [200,1600) bytes.\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8b7acf768f64c7283266190506e7cd49",
     "grade": true,
     "grade_id": "exercise-2-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `app_bytes_stats` function\n",
    "\"\"\"\n",
    "\n",
    "# Call `app_bytes_stats` function\n",
    "stats = app_bytes_stats(data)\n",
    "\n",
    "assert_equal(0, stats[0])\n",
    "assert_equal(2330, stats[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1f3203b240d8ee2eea17f79bb2fa2bbe",
     "grade": false,
     "grade_id": "exercise-2-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.3 (5 points)\n",
    "\n",
    "Implement the function <code>**get_most_dangerous_servers**</code> below, which takes as input a <code>**pandas.DataFrame**</code> and an integer <code><b>k</b></code>, and returns an **ordered list** of <code><b>k</b></code> elements, where each element of is a tuple containing the name of the server (i.e., the <code>**SERVER**</code> field) and the *probability* of that server being involved in malicious web traffic. The final list shall be ordered by probability of maliciousness (not-ascending) and, within that, lexicographically sorted (not-descending).\n",
    "\n",
    "(**SUGGESTION:** In order to answer this question, you will need to compute the probability of maliciousness fo each **group** of servers.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "2df3b8b0b6d07d59dfed5b00b2720954",
     "grade": false,
     "grade_id": "exercise-2-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def get_most_dangerous_servers(data, k):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "28f8212173cb4ab43a74b9d1112309eb",
     "grade": true,
     "grade_id": "exercise-2-3-test",
     "locked": true,
     "points": 5,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the implementation of the `get_most_dangerous_servers` function\n",
    "\"\"\"\n",
    "\n",
    "# Call `get_most_dangerous_servers` function\n",
    "most_dangerous_servers = get_most_dangerous_servers(data, 15)\n",
    "\n",
    "assert_equal(15, len(most_dangerous_servers))\n",
    "assert_equal(\"Apache/1.3.31 (Unix) PHP/4.3.9 mod_perl/1.29 rus/PL30.20\", most_dangerous_servers[1][0])\n",
    "assert_equal(1.0, most_dangerous_servers[3][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "078359a02c74006c5c57c79ed11339e1",
     "grade": false,
     "grade_id": "exercise-2-4-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "## Exercise 2.4 (7 points)\n",
    "\n",
    "This exercise is made of **3** main questions, which you can answer independently to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "85d3a3a324cd98adcc73e057dd825587",
     "grade": false,
     "grade_id": "exercise-2-4-1-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 1 (1 point)\n",
    "\n",
    "Feature <code>**CHARSET**</code> represents a categorical variable which can take on <b>5</b> distinct values (excluding NAs).\n",
    "Assign to the variable <code>**us_ascii**</code> below the total number of records in the dataset whose charset is equal to <code><b>us-ascii</b></code>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ffe4ec71af5fb71fdf1989cf79e7c318",
     "grade": false,
     "grade_id": "exercise-2-4-1-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "us_ascii = None\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "346aa0feeb3d40495b1394669750ef6f",
     "grade": true,
     "grade_id": "exercise-2-4-1-test",
     "locked": true,
     "points": 1,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of the `us_ascii`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (us_ascii == None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4945df643702eac890e6fe1742d544ee",
     "grade": false,
     "grade_id": "exercise-2-4-2-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 2 (3 points)\n",
    "\n",
    "Plot the histogram along with the empirical density of the <code>**CONTENT_LENGTH**</code> using <code>**sns.distplot**</code>, and assign the result of the plot to the variable <code>**dist_plot**</code>. \n",
    "\n",
    "In addition to that, compute both the <code>**skewness**</code> and the <code>**kurtosis**</code> of the distribution. Those measure the simmetry and the \"thickness\" of the tail of the distribution. Given a sample of $N$ i.i.d. observations of a single random variable $X$, i.e., $x_1, \\ldots, x_N$, a possible way to compute those values is as follows:\n",
    "\n",
    "$$\n",
    "\\text{skewness} = \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^3}{s^3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{kurtosis} = \\frac{\\frac{1}{N}\\sum_{i=1}^N (x_i - \\bar{x})^4}{s^4} - 3\n",
    "$$\n",
    "where $\\bar{x}$ and $s$ are the sample mean and the sample standard deviation of $x_1, \\ldots, x_N$, respectively.\n",
    "\n",
    "(**NOTE:** Remember to correctly handle any possible missing values, e.g., call the <code><b>dropna()</b></code> method if needed...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "ee4c3260ff013d51604809951a14e408",
     "grade": false,
     "grade_id": "exercise-2-4-2-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "dist_plot = None # assign this to the outcome of sns.distplot call\n",
    "skewness = None # assign this to the value of skewness\n",
    "kurtosis = None # assign this to the value of kurtosis\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7be66831808917a6a1fbc0f0a0da78a5",
     "grade": true,
     "grade_id": "exercise-2-4-2-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `dist_plot`, `skewness`, and `kurtosis`\n",
    "\"\"\"\n",
    "\n",
    "assert_equal(False, (dist_plot == None))\n",
    "assert_equal(False, (skewness == None))\n",
    "assert_equal(False, (kurtosis == None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4aa2d3a02c6240c67d0b63a1cd2a996a",
     "grade": false,
     "grade_id": "exercise-2-4-3-text",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "### Question 3 (3 points)\n",
    "\n",
    "Implement the function <code>**n_right_num_spec_chars_outliers**</code> below, which takes as input a <code>**pandas.DataFrame**</code> object and returns the number of right outliers of the column <code>**NUMBER_SPECIAL_CHARACTERS**</code> (if any).<br />\n",
    "Any data point **less than** (resp., greater than) of the left (resp., right) fence is considered an outlier. Both left and right fences are empirically computed as follows:\n",
    "\n",
    "$$\n",
    "F_\\textrm{left} = Q_1 - 1.5 * \\texttt{IQR};~~F_\\textrm{right} = Q_3 + 1.5 * \\texttt{IQR}\n",
    "$$\n",
    "\n",
    "where $Q_1$ and $Q_3$ represents the 1st and 3rd quartile of the distribution of interest **without considering NAs**, and $\\texttt{IQR} = Q_3 - Q_1$.\n",
    "\n",
    "(**SUGGESTIONS:** Start from drawing the box plot and visually check whether there is any outlier or not. You can either invoke the <code>**quantile**</code> function defined on a <code>**pandas.Series**</code> object **or** use the <code>**numpy.percentile**</code> function which takes as input a <code>**pandas.Series**</code> object or, more generally, any object that can easily be converted into a <code>**numpy.array**</code>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "59416c1860e38e926f45f758a3a71f8c",
     "grade": false,
     "grade_id": "exercise-2-4-3-code",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "box_plot = None # assign this to the outcome of sns.boxplot call\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "def n_right_num_spec_chars_outliers(data):\n",
    "    \"\"\"\n",
    "    Returns the number of right outliers of the column `NUMBER_SPECIAL_CHARACTERS`\n",
    "    \"\"\"\n",
    "    fence_right = None # value of the right fence\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b711bc6ba83412be78a3ee47fb38827e",
     "grade": true,
     "grade_id": "exercise-2-4-3-test",
     "locked": true,
     "points": 3,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Test the correctness of `n_right_num_spec_chars_outliers`\n",
    "\"\"\"\n",
    "\n",
    "# Call `n_right_num_spec_chars_outliers` function\n",
    "outliers = n_right_num_spec_chars_outliers(data)\n",
    "\n",
    "assert_equal(False, (box_plot == None))\n",
    "assert_equal(False, (outliers == None))\n",
    "assert_equal(True, (outliers > 10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
